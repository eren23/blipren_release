{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpM07vQvBZfO"
      },
      "outputs": [],
      "source": [
        "# !pip -q install transformers accelerate datasets safetensors pillow requests\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_DSQA_YGW46"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6X9_fHXSCbU",
        "outputId": "d1382ea7-eec7-43b7-a3c5-eb940699a830"
      },
      "outputs": [],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "99376d9712094f8295124e56d635a4fb",
            "b04bdd13b64a472486b820afaeb78cf1",
            "a9b3f40212f340ecb5b3581b0c49b861",
            "801806894fbe4e09a42151932906b940",
            "5e67463a464c47d8acc2a3829eac8fc3",
            "6b8d9439974f4884b18b7c1240ff7ab7",
            "799fdfbf8f914ddbbeba4965af3a4972",
            "b95ccac6fb714a10b134800f043ba4e5",
            "b2abcf67e1264b83af7f0850b8d46750",
            "392b91a6283c4b1294cbc192983cef53",
            "02fdb5c9871c44ebbc4a190969e3494d",
            "a90849e2567845ffabe7a71902f10f2a",
            "6803d0c5e15f49ccbd7caf1aef83205a",
            "79d8ad1f0a8a47c0ac91c29445655edb",
            "28b4a6667a54470c881590960e4e67e8",
            "b46915a4e08a4d1a8c70e22e21e0942d",
            "666c155bb25447e9ae68618d21546c6c",
            "6e24aff13a684a33a317d3c4b8d626e3",
            "e23efe05c1af4de6be4744df2bf302cb",
            "dc405637c9264a7d99738dfb82dcb6ef"
          ]
        },
        "id": "rwVhwq89SAZ8",
        "outputId": "e5074510-b19d-4ee3-b914-78443fe3a333"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6O3l-9jG7d3"
      },
      "outputs": [],
      "source": [
        "import os, math, random, io, requests, json, time, torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, Dataset, Image as HFImage\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    SiglipVisionModel,\n",
        ")\n",
        "from torch.amp import autocast, GradScaler\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "\n",
        "# CONFIG\n",
        "\n",
        "LLM_NAME    = \"meta-llama/Llama-3.2-1B\"\n",
        "VISION_NAME = \"google/siglip-so400m-patch14-384\"\n",
        "\n",
        "cache_dir        = \"/content/livis_cache\"\n",
        "max_cached_items = 100000      # max (train+val) cached image+caption pairs\n",
        "train_size       = 95000\n",
        "val_size         = 5000       # train + val = 50k\n",
        "\n",
        "batch_size   = 32\n",
        "max_txt_len  = 128\n",
        "total_steps  = 50000\n",
        "warmup_steps = 1000\n",
        "grad_accum   = 8\n",
        "val_interval = 1000\n",
        "save_step_10k = 10000\n",
        "save_step_5k = 5000\n",
        "\n",
        "\n",
        "use_wandb   = True\n",
        "wandb_project = \"blip2-llama-livis-50k\"\n",
        "\n",
        "# resume flags (only affect Q-Former + Projector)\n",
        "RESUME = False\n",
        "RESUME_QFORMER_PATH   = \"checkpoints/qformer_best.pt\"\n",
        "RESUME_PROJECTOR_PATH = \"checkpoints/projector_best.pt\"\n",
        "\n",
        "device = \"cuda\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "print(\"Device:\", device)\n",
        "\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "existing_jpgs = [f for f in os.listdir(cache_dir) if f.lower().endswith(\".jpg\")]\n",
        "print(f\"Found existing cached JPGs in {cache_dir}: {len(existing_jpgs)}\")\n",
        "\n",
        "\n",
        "# LOAD LLM (LLaMA-3.2-1B)\n",
        "\n",
        "print(f\"Loading LLM: {LLM_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "llm.eval()\n",
        "\n",
        "d_model = llm.config.hidden_size\n",
        "print(\"LLM hidden size:\", d_model)\n",
        "\n",
        "\n",
        "# LOAD SIGLIP VISION ENCODER\n",
        "\n",
        "print(f\"Loading vision encoder: {VISION_NAME}\")\n",
        "vision_model = SiglipVisionModel.from_pretrained(\n",
        "    VISION_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)\n",
        "processor = AutoProcessor.from_pretrained(VISION_NAME)\n",
        "vision_model.eval()\n",
        "\n",
        "d_vision = vision_model.config.hidden_size\n",
        "resize_size = processor.image_processor.size.get(\"shortest_edge\", 384)\n",
        "print(\"SigLIP hidden size:\", d_vision)\n",
        "print(\"Resize size:\", resize_size)\n",
        "\n",
        "# Pre-compute patch grid size for attention visualization\n",
        "img_size  = getattr(vision_model.config, \"image_size\", 384)\n",
        "patch_sz  = getattr(vision_model.config, \"patch_size\", 14)\n",
        "grid_H = img_size // patch_sz\n",
        "grid_W = img_size // patch_sz\n",
        "\n",
        "\n",
        "# CACHE LIVIS DATASET LOCALLY \n",
        "print(\"Loading raw LIVIS dataset from Hugging Face...\")\n",
        "raw = load_dataset(\"laion/220k-GPT4Vision-captions-from-LIVIS\", split=\"train\")\n",
        "print(\"Raw dataset size:\", len(raw))\n",
        "\n",
        "def extract_caption(ex):\n",
        "    \"\"\"\n",
        "    Prefer short_caption; fallback to caption; always prefix 'Short caption:' so\n",
        "    the model learns consistent prompting. (Matches inference prompt.)\n",
        "    \"\"\"\n",
        "    sc = (ex.get(\"short_caption\") or \"\").strip()\n",
        "    if not sc:\n",
        "        sc = (ex.get(\"caption\") or \"An image.\").strip()\n",
        "    return f\"Short caption: {sc}\"\n",
        "\n",
        "paths_ok = []\n",
        "caps_ok  = []\n",
        "jobs     = []  # (idx, url, caption) to be downloaded\n",
        "\n",
        "print(f\"Scanning dataset to reuse cache & queue missing images (target {max_cached_items})...\")\n",
        "\n",
        "for idx, ex in enumerate(raw):\n",
        "    if len(paths_ok) + len(jobs) >= max_cached_items:\n",
        "        break\n",
        "\n",
        "    url = ex.get(\"url\")\n",
        "    if not url:\n",
        "        continue\n",
        "\n",
        "    cap = extract_caption(ex)\n",
        "    img_path = os.path.join(cache_dir, f\"{idx:06d}.jpg\")\n",
        "    txt_path = os.path.join(cache_dir, f\"{idx:06d}.txt\")\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        # Try to reuse existing image; if broken, schedule for re-download\n",
        "        try:\n",
        "            Image.open(img_path).verify()\n",
        "            if not os.path.exists(txt_path):\n",
        "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(cap)\n",
        "            paths_ok.append(img_path)\n",
        "            caps_ok.append(cap)\n",
        "        except Exception:\n",
        "            jobs.append((idx, url, cap))\n",
        "        continue\n",
        "\n",
        "    # Missing: schedule for download\n",
        "    jobs.append((idx, url, cap))\n",
        "\n",
        "print(f\"Already valid images (reused): {len(paths_ok)}\")\n",
        "print(f\"Images queued for download:   {len(jobs)}\")\n",
        "\n",
        "# Clip job list so final count <= max_cached_items\n",
        "if len(paths_ok) > max_cached_items:\n",
        "    paths_ok = paths_ok[:max_cached_items]\n",
        "    caps_ok  = caps_ok [:max_cached_items]\n",
        "    jobs = []\n",
        "elif len(paths_ok) + len(jobs) > max_cached_items:\n",
        "    jobs = jobs[: max_cached_items - len(paths_ok)]\n",
        "\n",
        "print(f\"Final plan -> keep {len(paths_ok)} existing + download {len(jobs)} new.\")\n",
        "\n",
        "def fetch_one(job):\n",
        "    \"\"\"\n",
        "    Worker for parallel download.\n",
        "    job: (idx, url, caption)\n",
        "    Returns (img_path, caption) or None if failed.\n",
        "    \"\"\"\n",
        "    idx, url, cap = job\n",
        "    img_path = os.path.join(cache_dir, f\"{idx:06d}.jpg\")\n",
        "    txt_path = os.path.join(cache_dir, f\"{idx:06d}.txt\")\n",
        "\n",
        "    # If already exists and ok, just ensure caption\n",
        "    if os.path.exists(img_path):\n",
        "        try:\n",
        "            Image.open(img_path).verify()\n",
        "            if not os.path.exists(txt_path):\n",
        "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(cap)\n",
        "            return (img_path, cap)\n",
        "        except Exception:\n",
        "            pass  # will re-download\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=7)\n",
        "        img = Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
        "        img.save(img_path, \"JPEG\", quality=92)\n",
        "\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(cap)\n",
        "\n",
        "        return (img_path, cap)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "if jobs:\n",
        "    print(\"üöÄ Parallel downloading with 20 workers...\")\n",
        "    max_workers = 100\n",
        "    done = 0\n",
        "    good = 0\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex_pool:\n",
        "        futures = {ex_pool.submit(fetch_one, j): j[0] for j in jobs}\n",
        "        for fut in as_completed(futures):\n",
        "            res = fut.result()\n",
        "            done += 1\n",
        "            if res is not None:\n",
        "                p, c = res\n",
        "                paths_ok.append(p)\n",
        "                caps_ok.append(c)\n",
        "                good += 1\n",
        "            if done % 500 == 0:\n",
        "                print(f\"[Download progress] finished={done}/{len(jobs)}, successful={good}\")\n",
        "\n",
        "print(f\"‚úÖ Cache ready. Total valid pairs: {len(paths_ok)}\")\n",
        "\n",
        "if len(paths_ok) < (train_size + val_size):\n",
        "    raise RuntimeError(\n",
        "        f\"Not enough cached images+captions ({len(paths_ok)}) \"\n",
        "        f\"for requested train+val={train_size+val_size}.\"\n",
        "    )\n",
        "\n",
        "# Trim to exactly train+val\n",
        "paths_ok = paths_ok[:train_size + val_size]\n",
        "caps_ok  = caps_ok [:train_size + val_size]\n",
        "\n",
        "print(\"Building local HF dataset from cached files...\")\n",
        "\n",
        "hf_ds = Dataset.from_dict({\n",
        "    \"image\": paths_ok,\n",
        "    \"caption\": caps_ok,\n",
        "})\n",
        "hf_ds = hf_ds.cast_column(\"image\", HFImage())  # lazy PIL loading\n",
        "\n",
        "print(\"Full cached dataset size:\", len(hf_ds))\n",
        "print(\"Example row:\", hf_ds[0])\n",
        "\n",
        "\n",
        "# TRAIN / VAL SPLIT (48k / 2k)\n",
        "\n",
        "hf_ds = hf_ds.shuffle(seed=42)\n",
        "train_ds = hf_ds.select(range(train_size))\n",
        "val_ds   = hf_ds.select(range(train_size, train_size + val_size))\n",
        "\n",
        "print(\"Train size:\", len(train_ds))\n",
        "print(\"Val size:  \", len(val_ds))\n",
        "\n",
        "\n",
        "# PYTORCH DATALOADERS\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    caps   = []\n",
        "    for ex in batch:\n",
        "        img = ex[\"image\"]\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = img.convert(\"RGB\")\n",
        "        else:\n",
        "            # HF Image returns PIL.Image upon access; keep robust\n",
        "            try:\n",
        "                img = img.convert(\"RGB\")\n",
        "            except Exception:\n",
        "                continue\n",
        "        images.append(img)\n",
        "        caps.append(ex[\"caption\"])\n",
        "\n",
        "    if len(images) == 0:\n",
        "        return None\n",
        "\n",
        "    pixel_values = processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "    enc = tokenizer(\n",
        "        caps,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_txt_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": enc[\"input_ids\"],\n",
        "        \"attention_mask\": enc[\"attention_mask\"],\n",
        "        \"captions\": caps,\n",
        "        \"pil_images\": images,\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(\"Dataloaders ready.\")\n",
        "\n",
        "\n",
        "# Q-FORMER\n",
        "\n",
        "class QFormerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    One block of a BLIP2-style Q-Former:\n",
        "      - LN + self-attention on query tokens\n",
        "      - LN + cross-attention from queries ‚Üí vision tokens\n",
        "      - LN + MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "        hidden = int(d_model * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, q, v, attn_mask_v=None):\n",
        "        # Self-attention on q\n",
        "        q2, _ = self.self_attn(self.ln1(q), self.ln1(q), self.ln1(q), need_weights=False)\n",
        "        q = q + q2\n",
        "\n",
        "        # Cross-attention (queries = q, keys/values = v)\n",
        "        # attn_mask_v: optional mask over vision tokens (not used here)\n",
        "        q2, attn = self.cross_attn(\n",
        "            self.ln2(q), self.ln2(v), self.ln2(v),\n",
        "            key_padding_mask=attn_mask_v,\n",
        "            need_weights=True,\n",
        "            average_attn_weights=False,\n",
        "        )\n",
        "        q = q + q2\n",
        "\n",
        "        # Feed-forward\n",
        "        q = q + self.mlp(self.ln3(q))\n",
        "\n",
        "        return q, attn\n",
        "\n",
        "\n",
        "class QFormer(nn.Module):\n",
        "    def __init__(self, d_vis, d_model,\n",
        "                 n_queries=32,\n",
        "                 n_layers=8,\n",
        "                 heads=8,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.query = nn.Parameter(torch.randn(n_queries, d_model))\n",
        "        self.vis_proj = nn.Linear(d_vis, d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            QFormerBlock(d_model, heads, mlp_ratio=mlp_ratio, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.final_ln = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.last_attn = None  # store last layer attention for visualization\n",
        "\n",
        "    def forward(self, vis_tokens: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        vis_tokens: (B, N, d_vis)\n",
        "        returns:    (B, K, d_model)\n",
        "        \"\"\"\n",
        "        v = self.vis_proj(vis_tokens)          # (B, N, d_model)\n",
        "\n",
        "        B = v.size(0)\n",
        "        q = self.query.unsqueeze(0).expand(B, -1, -1)  # (B, K, d_model)\n",
        "\n",
        "        last_attn = None\n",
        "        for blk in self.layers:\n",
        "            q, attn = blk(q, v, attn_mask_v=None)      # attn: (B, heads, K, N)\n",
        "            last_attn = attn\n",
        "\n",
        "        self.last_attn = last_attn\n",
        "        q = self.final_ln(q)\n",
        "        return q\n",
        "\n",
        "qformer = QFormer(\n",
        "    d_vis=d_vision,\n",
        "    d_model=d_model,\n",
        "    n_queries=32,\n",
        "    n_layers=8,      # deeper than before\n",
        "    heads=8,\n",
        "    mlp_ratio=4.0,\n",
        "    dropout=0.1,\n",
        ").to(device)\n",
        "\n",
        "print(\"Q-Former params (M):\", sum(p.numel() for p in qformer.parameters())/1e6)\n",
        "\n",
        "\n",
        "# PROJECTOR (LN + 2-layer MLP + residual)\n",
        "\n",
        "class Projector(nn.Module):\n",
        "    \"\"\"\n",
        "    Stronger projector to align Q-Former outputs with LLM embedding space:\n",
        "      y = LN(x + MLP(LN(x)))\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden = int(d_model * mlp_ratio)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.ln1(x)\n",
        "        x = self.mlp(x)\n",
        "        x = residual + x\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "projector = Projector(d_model, mlp_ratio=4.0, dropout=0.1).to(device)\n",
        "print(\"Projector params (M):\", sum(p.numel() for p in projector.parameters())/1e6)\n",
        "\n",
        "\n",
        "# BLIP-2 WRAPPER (visual prefix + text)\n",
        "\n",
        "class BLIP2(nn.Module):\n",
        "    def __init__(self, llm, vision, qformer, projector):\n",
        "        super().__init__()\n",
        "        self.llm = llm\n",
        "        self.vision = vision\n",
        "        self.qformer = qformer\n",
        "        self.projector = projector\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask):\n",
        "        # Vision encoder (frozen, fp16)\n",
        "        with torch.no_grad():\n",
        "            vout = self.vision(pixel_values=pixel_values)\n",
        "            vtoks = vout.last_hidden_state[:, 1:, :]   # drop CLS ‚Üí (B, N-1, d_vision)\n",
        "\n",
        "        # Q-Former + projector in fp32, then cast to LLM dtype\n",
        "        q = self.qformer(vtoks.to(torch.float32))      # (B, K, d_model)\n",
        "        q = self.projector(q).to(self.llm.dtype)       # (B, K, d_model)\n",
        "        K = q.size(1)\n",
        "\n",
        "        # Text embeddings\n",
        "        embed = self.llm.get_input_embeddings()\n",
        "        txt   = embed(input_ids)                       # (B, T, d_model)\n",
        "\n",
        "        # Concatenate visual prefix + text\n",
        "        all_emb = torch.cat([q, txt], dim=1)           # (B, K+T, d_model)\n",
        "\n",
        "        # Build attention mask\n",
        "        prefix_mask = torch.ones(\n",
        "            input_ids.size(0), K,\n",
        "            device=input_ids.device,\n",
        "            dtype=attention_mask.dtype,\n",
        "        )\n",
        "        full_mask = torch.cat([prefix_mask, attention_mask], dim=1)  # (B, K+T)\n",
        "\n",
        "        # Labels: ignore visual prefix\n",
        "        labels = input_ids.clone()\n",
        "        labels[attention_mask == 0] = -100\n",
        "        prefix_labels = torch.full(\n",
        "            (input_ids.size(0), K),\n",
        "            -100,\n",
        "            device=input_ids.device,\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        full_labels = torch.cat([prefix_labels, labels], dim=1)\n",
        "\n",
        "        out = self.llm(\n",
        "            inputs_embeds=all_emb,\n",
        "            attention_mask=full_mask,\n",
        "            labels=full_labels,\n",
        "        )\n",
        "        return out.logits, out.loss\n",
        "\n",
        "model = BLIP2(llm, vision_model, qformer, projector)\n",
        "\n",
        "# Freeze LLM + Vision (only Q-Former & Projector train)\n",
        "for p in llm.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in vision_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "# RESUME FROM WEIGHTS (OPTIONAL)\n",
        "\n",
        "if RESUME:\n",
        "    print(\"üîÅ RESUME is True ‚Äì trying to load Q-Former & Projector weights...\")\n",
        "    if os.path.isfile(RESUME_QFORMER_PATH):\n",
        "        try:\n",
        "            qformer.load_state_dict(torch.load(RESUME_QFORMER_PATH, map_location=device))\n",
        "            print(f\"Loaded Q-Former from {RESUME_QFORMER_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Failed to load Q-Former weights:\", e)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Q-Former resume path not found: {RESUME_QFORMER_PATH}\")\n",
        "\n",
        "    if os.path.isfile(RESUME_PROJECTOR_PATH):\n",
        "        try:\n",
        "            projector.load_state_dict(torch.load(RESUME_PROJECTOR_PATH, map_location=device))\n",
        "            print(f\"Loaded Projector from {RESUME_PROJECTOR_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Failed to load Projector weights:\", e)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Projector resume path not found: {RESUME_PROJECTOR_PATH}\")\n",
        "\n",
        "\n",
        "# OPTIMIZER + SCHEDULER + AMP\n",
        "\n",
        "train_params = list(qformer.parameters()) + list(projector.parameters())\n",
        "optimizer = torch.optim.AdamW(train_params, lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    ratio = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    return max(0.1, 0.5 * (1 + math.cos(math.pi * ratio)))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "scaler = GradScaler(device=\"cuda\")\n",
        "\n",
        "if use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, config={\n",
        "        \"llm\": LLM_NAME,\n",
        "        \"vision\": VISION_NAME,\n",
        "        \"lr\": 5e-5,\n",
        "        \"warmup\": warmup_steps,\n",
        "        \"steps\": total_steps,\n",
        "        \"grad_accum\": grad_accum,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"train_size\": train_size,\n",
        "        \"val_size\": val_size,\n",
        "        \"cache_dir\": cache_dir,\n",
        "    })\n",
        "\n",
        "\n",
        "# INFERENCE HELPER (single image ‚Üí caption)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_caption_for_image(img: Image.Image, max_new_tokens=32):\n",
        "    img = img.convert(\"RGB\")\n",
        "    v = processor(images=img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    vout = vision_model(pixel_values=v)\n",
        "    vtoks = vout.last_hidden_state[:, 1:, :]\n",
        "\n",
        "    q = qformer(vtoks.to(torch.float32))\n",
        "    q = projector(q).to(llm.dtype)\n",
        "    K = q.size(1)\n",
        "\n",
        "    prompt = \"Short caption: \"\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    ids = enc[\"input_ids\"]\n",
        "    txt_emb = llm.get_input_embeddings()(ids)\n",
        "\n",
        "    all_emb = torch.cat([q, txt_emb], dim=1)\n",
        "    attn_mask = torch.ones(1, all_emb.size(1), device=device, dtype=torch.long)\n",
        "\n",
        "    out_ids = llm.generate(\n",
        "        inputs_embeds=all_emb,\n",
        "        attention_mask=attn_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    if text.lower().startswith(prompt.lower()):\n",
        "        text = text[len(prompt):].strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# ATTENTION HEATMAP HELPER\n",
        "\n",
        "def save_attention_heatmap(img: Image.Image, step: int, max_queries: int = 4):\n",
        "    img = img.convert(\"RGB\")\n",
        "    v = processor(images=img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vout = vision_model(pixel_values=v)\n",
        "        vtoks = vout.last_hidden_state[:, 1:, :]\n",
        "        _ = qformer(vtoks.to(torch.float32))\n",
        "        attn = qformer.last_attn  # (B, heads, K, N)\n",
        "\n",
        "    if attn is None:\n",
        "        return None\n",
        "\n",
        "    # average over heads, take first batch\n",
        "    attn = attn.mean(dim=1)[0]  # (K, N)\n",
        "    K, N = attn.shape\n",
        "\n",
        "    H, W = grid_H, grid_W\n",
        "    if H * W > N:\n",
        "        H = int(math.sqrt(N)) or 1\n",
        "        W = H\n",
        "    attn = attn[:, :H * W].view(K, H, W)\n",
        "\n",
        "    num_q = min(max_queries, K)\n",
        "    fig, axes = plt.subplots(1, num_q, figsize=(4 * num_q, 4))\n",
        "    if num_q == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    for qi in range(num_q):\n",
        "        heat = attn[qi].detach().cpu().numpy()\n",
        "        heat = (heat - heat.min()) / (heat.max() - heat.min() + 1e-8)\n",
        "\n",
        "        ax = axes[qi]\n",
        "        ax.imshow(img_np)\n",
        "        ax.imshow(heat, cmap=\"jet\", alpha=0.45)\n",
        "        ax.set_title(f\"Query {qi}\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out_path = os.path.join(\"logs\", f\"attn_step_{step}.png\")\n",
        "    plt.savefig(out_path)\n",
        "    plt.close(fig)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# VALIDATION LOOP (console + jsonl + wandb)\n",
        "\n",
        "val_pred_log_path = os.path.join(cache_dir, \"val_predictions.jsonl\")\n",
        "\n",
        "def run_validation(global_step):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    example_logged = False\n",
        "    example_img = None\n",
        "    example_gt  = None\n",
        "    example_pred= None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids    = batch[\"input_ids\"].to(device)\n",
        "            attn_mask    = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            with autocast(\"cuda\", dtype=torch.float16):\n",
        "                _, loss = model(input_ids, pixel_values, attn_mask)\n",
        "\n",
        "            bs = input_ids.size(0)\n",
        "            total_loss += loss.item() * bs\n",
        "            count += bs\n",
        "\n",
        "            if not example_logged:\n",
        "                img0 = batch[\"pil_images\"][0]\n",
        "                gt0  = batch[\"captions\"][0]\n",
        "                pred0 = generate_caption_for_image(img0)\n",
        "                example_img = img0\n",
        "                example_gt  = gt0\n",
        "                example_pred= pred0\n",
        "                example_logged = True\n",
        "\n",
        "    avg_loss = total_loss / max(1, count)\n",
        "    print(f\"[VAL @ step {global_step}] loss={avg_loss:.4f}\")\n",
        "    if example_logged:\n",
        "        print(f\"[VAL] GT : {example_gt}\")\n",
        "        print(f\"[VAL] PR : {example_pred}\")\n",
        "\n",
        "    # Append example GT+Pred to jsonl log\n",
        "    if example_logged:\n",
        "        try:\n",
        "            with open(val_pred_log_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(json.dumps({\n",
        "                    \"step\": global_step,\n",
        "                    \"gt\": example_gt,\n",
        "                    \"pred\": example_pred,\n",
        "                }) + \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error writing val prediction log:\", e)\n",
        "\n",
        "    attn_path = None\n",
        "    if example_img is not None:\n",
        "        attn_path = save_attention_heatmap(example_img, global_step)\n",
        "\n",
        "    if use_wandb:\n",
        "        import wandb\n",
        "        log_dict = {\"val_loss\": avg_loss, \"step\": global_step}\n",
        "\n",
        "        # Single-row table with GT+Pred\n",
        "        table = wandb.Table(columns=[\"step\", \"image\", \"gt\", \"pred\"])\n",
        "        if example_img is not None:\n",
        "            table.add_data(\n",
        "                global_step,\n",
        "                wandb.Image(example_img),\n",
        "                example_gt,\n",
        "                example_pred,\n",
        "            )\n",
        "        log_dict[\"val_samples\"] = table\n",
        "\n",
        "        # Attention overlay image\n",
        "        if attn_path is not None and os.path.exists(attn_path):\n",
        "            log_dict[\"val_attention\"] = wandb.Image(\n",
        "                attn_path, caption=f\"Q-Former attention @ step {global_step}\"\n",
        "            )\n",
        "\n",
        "        wandb.log(log_dict, step=global_step)\n",
        "\n",
        "    model.train()\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# TRAINING LOOP (advanced logs + validation + checkpoints)\n",
        "\n",
        "print(\"üöÄ Training starting...\")\n",
        "best_val_loss = float(\"inf\")\n",
        "global_step = 0\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "running_loss = 0.0\n",
        "start_time = time.time()\n",
        "\n",
        "while global_step < total_steps:\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    if batch is None:\n",
        "        continue\n",
        "\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    input_ids    = batch[\"input_ids\"].to(device)\n",
        "    attn_mask    = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    with autocast(\"cuda\", dtype=torch.float16):\n",
        "        _, loss = model(input_ids, pixel_values, attn_mask)\n",
        "        loss = loss / grad_accum\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    # Occasionally log caption lengths histogram\n",
        "    if use_wandb and (global_step % 1000 == 0):\n",
        "        cap_lengths = attn_mask.sum(dim=1).detach().cpu().tolist()\n",
        "        import wandb\n",
        "        wandb.log(\n",
        "            {\"caption_lengths\": wandb.Histogram(cap_lengths)},\n",
        "            step=global_step,\n",
        "        )\n",
        "\n",
        "    grad_norm = None\n",
        "    if (global_step + 1) % grad_accum == 0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(train_params, 1.0).item()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_wandb:\n",
        "            import wandb\n",
        "            wandb.log({\"grad_norm\": grad_norm}, step=global_step)\n",
        "\n",
        "    # Training logs\n",
        "    if global_step % 200 == 0:\n",
        "        avg_train_loss = running_loss / max(1, (global_step + 1))\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "        elapsed = time.time() - start_time\n",
        "        steps_done = global_step + 1\n",
        "        eta_secs = (total_steps - steps_done) * (elapsed / steps_done)\n",
        "        eta_mins = eta_secs / 60.0\n",
        "\n",
        "        print(\n",
        "            f\"[{global_step:05d}/{total_steps}] \"\n",
        "            f\"loss={loss.item():.4f} (avg={avg_train_loss:.4f}) \"\n",
        "            f\"lr={lr:.6e} grad_norm={grad_norm} \"\n",
        "            f\"ETA={eta_mins:.1f} min\"\n",
        "        )\n",
        "\n",
        "        if use_wandb:\n",
        "            import wandb\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"train_loss\": loss.item(),\n",
        "                    \"train_loss_avg\": avg_train_loss,\n",
        "                    \"lr\": lr,\n",
        "                    \"eta_min\": eta_mins,\n",
        "                    \"step\": global_step,\n",
        "                },\n",
        "                step=global_step,\n",
        "            )\n",
        "\n",
        "    # Validation\n",
        "    if (global_step + 1) % val_interval == 0:\n",
        "        val_loss = run_validation(global_step + 1)\n",
        "        if use_wandb:\n",
        "            import wandb\n",
        "            avg_train_loss = running_loss / max(1, (global_step + 1))\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"train_val_gap\": avg_train_loss - val_loss\n",
        "                },\n",
        "                step=global_step + 1,\n",
        "            )\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            try:\n",
        "                os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "                torch.save(qformer.state_dict(), \"checkpoints/qformer_best.pt\")\n",
        "                torch.save(projector.state_dict(), \"checkpoints/projector_best.pt\")\n",
        "                print(f\"‚úÖ Saved new best checkpoint at step {global_step+1}, val_loss={val_loss:.4f}\")\n",
        "                if use_wandb:\n",
        "                    import wandb\n",
        "                    wandb.save(\"checkpoints/qformer_best.pt\")\n",
        "                    wandb.save(\"checkpoints/projector_best.pt\")\n",
        "            except Exception as e:\n",
        "                print(\"‚ö†Ô∏è Error while saving best checkpoint (ignored):\", e)\n",
        "\n",
        "    # Explicit save at 10k or 5k\n",
        "    if (global_step + 1) == save_step_10k or (global_step + 1) == save_step_5k:\n",
        "        try:\n",
        "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "            torch.save(qformer.state_dict(), \"checkpoints/qformer_step10k.pt\")\n",
        "            torch.save(projector.state_dict(), \"checkpoints/projector_step10k.pt\")\n",
        "            print(\"üíæ Saved 10k-step checkpoint.\")\n",
        "            if use_wandb:\n",
        "                import wandb\n",
        "                wandb.save(\"checkpoints/qformer_step10k.pt\")\n",
        "                wandb.save(\"checkpoints/projector_step10k.pt\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error while saving 10k-step checkpoint (ignored):\", e)\n",
        "\n",
        "    global_step += 1\n",
        "\n",
        "print(\"üéâ Training Finished!\")\n",
        "\n",
        "\n",
        "# QUICK INFERENCE EXAMPLE (after training)\n",
        "\n",
        "ex = random.choice(list(val_ds))\n",
        "img = ex[\"image\"]\n",
        "if isinstance(img, Image.Image):\n",
        "    img = img.convert(\"RGB\")\n",
        "pred = generate_caption_for_image(img)\n",
        "gt   = ex[\"caption\"]\n",
        "print(\"\\n[INFERENCE EXAMPLE]\")\n",
        "print(\"GT:\", gt)\n",
        "print(\"Pred:\", pred)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02fdb5c9871c44ebbc4a190969e3494d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28b4a6667a54470c881590960e4e67e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "392b91a6283c4b1294cbc192983cef53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e67463a464c47d8acc2a3829eac8fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_79d8ad1f0a8a47c0ac91c29445655edb",
            "style": "IPY_MODEL_28b4a6667a54470c881590960e4e67e8",
            "tooltip": ""
          }
        },
        "666c155bb25447e9ae68618d21546c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6803d0c5e15f49ccbd7caf1aef83205a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b8d9439974f4884b18b7c1240ff7ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b46915a4e08a4d1a8c70e22e21e0942d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_666c155bb25447e9ae68618d21546c6c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "6e24aff13a684a33a317d3c4b8d626e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e23efe05c1af4de6be4744df2bf302cb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc405637c9264a7d99738dfb82dcb6ef",
            "value": "Connecting..."
          }
        },
        "799fdfbf8f914ddbbeba4965af3a4972": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "79d8ad1f0a8a47c0ac91c29445655edb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "801806894fbe4e09a42151932906b940": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a90849e2567845ffabe7a71902f10f2a",
            "style": "IPY_MODEL_6803d0c5e15f49ccbd7caf1aef83205a",
            "value": true
          }
        },
        "99376d9712094f8295124e56d635a4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_799fdfbf8f914ddbbeba4965af3a4972"
          }
        },
        "a90849e2567845ffabe7a71902f10f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b3f40212f340ecb5b3581b0c49b861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_392b91a6283c4b1294cbc192983cef53",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_02fdb5c9871c44ebbc4a190969e3494d",
            "value": ""
          }
        },
        "b04bdd13b64a472486b820afaeb78cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95ccac6fb714a10b134800f043ba4e5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2abcf67e1264b83af7f0850b8d46750",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b2abcf67e1264b83af7f0850b8d46750": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b46915a4e08a4d1a8c70e22e21e0942d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b95ccac6fb714a10b134800f043ba4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc405637c9264a7d99738dfb82dcb6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e23efe05c1af4de6be4744df2bf302cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
