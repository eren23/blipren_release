{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3b41785cf6c34007815744ae8457eb09",
            "9fdd1b02fd2743c5b5a0387446aedb0e",
            "f569c7b14b70483b83e90a474c6b2cfc",
            "9c905ebd8dfb4bef9a93988b94465a7d",
            "0a85ee3222a245e5ad6275292567b441",
            "8cdac5c834604f8d8d8149bbb10b4a9c",
            "41b745e8e6f44c5c8318a7e7ad5b8ab7",
            "927e0c7f178f4caeab42da8cc7de9708",
            "a2020c428cf64c66990c1166d6a026c8",
            "93b8bd08c467410bbba5de44a808340e",
            "eff1deee387e4d46b6a5fd3cf878317c",
            "93203f4ef5a6423ca5aa64644eaa0595",
            "94ad11ac39c241b697d7b2ccad8248a3",
            "0f5682b48aad4696a8a58a05de0e5738",
            "55666f14af614893b8ccf1227d11775a",
            "4fcb41556b824761a9453ea1758e7652",
            "3974289c7526482797c9545f4e9b6831",
            "23610da04c044ecd8be542c1c807d122",
            "39f6dd8bb0ba4efb80a59186bc5a465e",
            "062767847423495bb66e01cafb26ac19"
          ]
        },
        "id": "qpaDJFJWa3J4",
        "outputId": "6feb568b-4001-4e57-b59f-81d9b936620b"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut707dcea5Sb",
        "outputId": "eb98141e-2667-4d1f-cb6c-a368e31ae078"
      },
      "outputs": [],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwBK5yOxRvRh",
        "outputId": "258a27d4-bb35-4e88-a2e1-47900c5cc762"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "\n",
        "# CONFIG\n",
        "LOCAL_DIR = \"/content/checkpoints\"\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "# WandB run ID that contains your checkpoint files\n",
        "RUN_PATH = \"eren23/blip2-llama-livis-50k/i7hxfev8\"\n",
        "\n",
        "# Filenames inside the run\n",
        "QFORMER_FILE   = \"checkpoints/qformer_step10k.pt\"\n",
        "PROJECTOR_FILE = \"checkpoints/projector_step10k.pt\"\n",
        "\n",
        "\n",
        "# Download from run files API\n",
        "print(\"ðŸ”µ Connecting to W&B API...\")\n",
        "api = wandb.Api()\n",
        "\n",
        "run = api.run(RUN_PATH)\n",
        "\n",
        "def download_run_file(filename, local_dir):\n",
        "    print(f\"â¬‡ Downloading {filename} ...\")\n",
        "    f = run.file(filename)\n",
        "    f.download(root=local_dir, replace=True)\n",
        "    local_path = os.path.join(local_dir, os.path.basename(filename))\n",
        "    print(f\"âœ” Saved to: {local_path}\")\n",
        "    return local_path\n",
        "\n",
        "q_path = download_run_file(QFORMER_FILE, LOCAL_DIR)\n",
        "p_path = download_run_file(PROJECTOR_FILE, LOCAL_DIR)\n",
        "\n",
        "print(\"\\nâœ… Downloads complete:\")\n",
        "print(\"Q-Former  â†’\", q_path)\n",
        "print(\"Projector â†’\", p_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEbXFVbb2Qul",
        "outputId": "235e958f-4dad-441e-8cab-68804386af5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# BLIP-2 Inference + Batch Attention Visualizations\n",
        "\n",
        "import os, math, torch\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    SiglipVisionModel,\n",
        ")\n",
        "\n",
        "\n",
        "# CONFIG\n",
        "\n",
        "device = \"cuda\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "LLM_NAME    = \"meta-llama/Llama-3.2-1B\"\n",
        "VISION_NAME = \"google/siglip-so400m-patch14-384\"\n",
        "\n",
        "# Point these to the weights you trained (e.g. step10k or best)\n",
        "QFORMER_CKPT  = \"/content/checkpoints/checkpoints/qformer_step10k.pt\"\n",
        "PROJECTOR_CKPT = \"/content/checkpoints/checkpoints/projector_step10k.pt\"\n",
        "\n",
        "ATTN_OUT_DIR = \"/content/attn_viz\"\n",
        "os.makedirs(ATTN_OUT_DIR, exist_ok=True)\n",
        "\n",
        "PROMPT_PREFIX = \"Short caption: \"\n",
        "\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "# LOAD LLM + TOKENIZER\n",
        "\n",
        "print(\"ðŸ§  Loading LLaMAâ€¦\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "llm.eval()\n",
        "d_model = llm.config.hidden_size\n",
        "print(\"LLM hidden size:\", d_model)\n",
        "\n",
        "\n",
        "# 2) LOAD SIGLIP VISION ENCODER\n",
        "\n",
        "print(\"ðŸ‘ Loading SigLIPâ€¦\")\n",
        "processor = AutoProcessor.from_pretrained(VISION_NAME)\n",
        "vision = SiglipVisionModel.from_pretrained(\n",
        "    VISION_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)\n",
        "vision.eval()\n",
        "d_vis = vision.config.hidden_size\n",
        "print(\"SigLIP hidden size:\", d_vis)\n",
        "\n",
        "\n",
        "# 3) Q-FORMER + PROJECTOR (MATCH TRAINING ARCH)\n",
        "#    - WITH ATTENTION STORAGE\n",
        "\n",
        "class QFormerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    One BLIP-2 style Q-Former block:\n",
        "      - LN + self-attn on queries\n",
        "      - LN + cross-attn (queries -> vision tokens)\n",
        "      - LN + MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "        hidden = int(d_model * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, q, v):\n",
        "        # q, v: (B, K, d), (B, N, d)\n",
        "        # self-attention\n",
        "        q2, _ = self.self_attn(\n",
        "            self.ln1(q), self.ln1(q), self.ln1(q),\n",
        "            need_weights=False\n",
        "        )\n",
        "        q = q + q2\n",
        "\n",
        "        # cross-attention: queries = q, keys/values = v\n",
        "        q2, attn = self.cross_attn(\n",
        "            self.ln2(q), self.ln2(v), self.ln2(v),\n",
        "            need_weights=True,\n",
        "            average_attn_weights=False,  # (B, heads, K, N)\n",
        "        )\n",
        "        q = q + q2\n",
        "\n",
        "        # MLP\n",
        "        q = q + self.mlp(self.ln3(q))\n",
        "        return q, attn\n",
        "\n",
        "\n",
        "class QFormer(nn.Module):\n",
        "    def __init__(self, d_vis, d_model,\n",
        "                 n_queries=32,\n",
        "                 n_layers=8,\n",
        "                 heads=8,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.query = nn.Parameter(torch.randn(n_queries, d_model))\n",
        "        self.vis_proj = nn.Linear(d_vis, d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            QFormerBlock(d_model, heads, mlp_ratio=mlp_ratio, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.final_ln = nn.LayerNorm(d_model)\n",
        "        self.last_attn = None  # (B, heads, K, N) from last block\n",
        "\n",
        "    def forward(self, vis_tokens):\n",
        "        # vis_tokens: (B, N, d_vis)\n",
        "        v = self.vis_proj(vis_tokens.to(torch.float32))   # (B, N, d_model)\n",
        "        B = v.size(0)\n",
        "        q = self.query.unsqueeze(0).expand(B, -1, -1)     # (B, K, d_model)\n",
        "\n",
        "        last_attn = None\n",
        "        for blk in self.layers:\n",
        "            q, attn = blk(q, v)\n",
        "            last_attn = attn\n",
        "\n",
        "        self.last_attn = last_attn  # (B, heads, K, N)\n",
        "        return self.final_ln(q)     # (B, K, d_model)\n",
        "\n",
        "\n",
        "class Projector(nn.Module):\n",
        "    \"\"\"\n",
        "    Strong projector used in training:\n",
        "      y = LN( x + MLP(LN(x)) )\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden = int(d_model * mlp_ratio)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r = x\n",
        "        x = self.ln1(x)\n",
        "        x = self.mlp(x)\n",
        "        x = r + x\n",
        "        return self.ln2(x)\n",
        "\n",
        "\n",
        "qformer = QFormer(d_vis, d_model).to(device)\n",
        "projector = Projector(d_model).to(device)\n",
        "\n",
        "print(\"ðŸ“¥ Loading Q-Former weights from:\", QFORMER_CKPT)\n",
        "qformer.load_state_dict(torch.load(QFORMER_CKPT, map_location=device))\n",
        "print(\"ðŸ“¥ Loading Projector weights from:\", PROJECTOR_CKPT)\n",
        "projector.load_state_dict(torch.load(PROJECTOR_CKPT, map_location=device))\n",
        "\n",
        "qformer.eval()\n",
        "projector.eval()\n",
        "\n",
        "\n",
        "# SMALL UTILS: TOKEN GRID SHAPE\n",
        "\n",
        "def tokens_to_grid(attn_1d: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Convert 1D attention over N tokens â†’ (H, W) grid.\n",
        "    Picks H,W so H*W divides N.\n",
        "    \"\"\"\n",
        "    N = attn_1d.numel()\n",
        "    h = int(math.sqrt(N))\n",
        "    while h > 1 and (N % h) != 0:\n",
        "        h -= 1\n",
        "    w = N // h\n",
        "    return attn_1d[:h * w].reshape(h, w)\n",
        "\n",
        "\n",
        "\n",
        "# CORE ENCODING (VISION + QFORMER + PROJECTOR)\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_batch(images):\n",
        "    \"\"\"\n",
        "    images: list[ PIL.Image ]\n",
        "    returns:\n",
        "      q_proj: (B, K, d_model) projected queries\n",
        "      attn:   (B, heads, K, N) last cross-attn maps\n",
        "    \"\"\"\n",
        "    imgs = [im.convert(\"RGB\") for im in images]\n",
        "    v = processor(images=imgs, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    vout = vision(pixel_values=v)\n",
        "    vtoks = vout.last_hidden_state[:, 1:, :]   # drop CLS â†’ (B, N, d_vis)\n",
        "\n",
        "    q = qformer(vtoks)                         # (B, K, d_model)\n",
        "    attn = qformer.last_attn                  # (B, heads, K, N)\n",
        "\n",
        "    q_proj = projector(q).to(llm.dtype)        # (B, K, d_model)\n",
        "    return q_proj, attn\n",
        "\n",
        "\n",
        "\n",
        "# BATCH GENERATION\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_with_attention(\n",
        "    images,\n",
        "    batch_tag=\"batch0\",\n",
        "    max_new_tokens=64,\n",
        "    num_queries_vis=4\n",
        "):\n",
        "    \"\"\"\n",
        "    images: list[ PIL.Image ]\n",
        "    Returns:\n",
        "      captions: list[str]\n",
        "      attn_paths: dict with keys:\n",
        "          'per_query_grid', 'per_image_accum_grid',\n",
        "          'per_image_single_accum_paths' (list of per-image pngs)\n",
        "    Also saves multiple visualization PNGs under ATTN_OUT_DIR.\n",
        "    \"\"\"\n",
        "    B = len(images)\n",
        "    assert B > 0\n",
        "\n",
        "    q_proj, attn = encode_batch(images)  # q_proj: (B,K,d), attn: (B,heads,K,N)\n",
        "    B, heads, K, N = attn.shape\n",
        "\n",
        "    # Text prefix for all samples\n",
        "    prompts = [PROMPT_PREFIX] * B\n",
        "    tok = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "    txt_emb = llm.get_input_embeddings()(tok[\"input_ids\"])   # (B, T, d)\n",
        "    T = txt_emb.size(1)\n",
        "\n",
        "    # Build prefix+text embeddings\n",
        "    full_emb = torch.cat([q_proj, txt_emb], dim=1)           # (B, K+T, d)\n",
        "    attn_mask = torch.ones(B, K + T, device=device, dtype=torch.long)\n",
        "\n",
        "    gen_ids = llm.generate(\n",
        "        inputs_embeds=full_emb,\n",
        "        attention_mask=attn_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode + strip prompt\n",
        "    captions = []\n",
        "    for i in range(B):\n",
        "        text = tokenizer.decode(gen_ids[i], skip_special_tokens=True)\n",
        "        if text.startswith(PROMPT_PREFIX):\n",
        "            text = text[len(PROMPT_PREFIX):].strip()\n",
        "        captions.append(text)\n",
        "\n",
        "    # Save attention visualizations\n",
        "    attn_paths = save_attention_visuals(\n",
        "        images, attn, batch_tag=batch_tag, num_queries_vis=num_queries_vis\n",
        "    )\n",
        "\n",
        "    return captions, attn_paths\n",
        "\n",
        "\n",
        "# ATTENTION VISUALIZATION (WITH UPSAMPLING)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def upsample_to_image(grid: np.ndarray, img_np: np.ndarray):\n",
        "    \"\"\"\n",
        "    Upsample a (hÃ—w) attention grid to match (HÃ—W) of the image.\n",
        "    Uses bilinear interpolation for smooth scaling.\n",
        "    \"\"\"\n",
        "    h, w = grid.shape\n",
        "    H, W = img_np.shape[:2]\n",
        "\n",
        "    t = torch.from_numpy(grid).float().unsqueeze(0).unsqueeze(0)  # (1,1,h,w)\n",
        "    t_up = F.interpolate(t, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "    return t_up.squeeze().numpy()  # (H, W)\n",
        "\n",
        "\n",
        "def save_attention_visuals(images, attn, batch_tag=\"batch0\", num_queries_vis=4):\n",
        "    \"\"\"\n",
        "    images: list[ PIL.Image ]\n",
        "    attn:   (B, heads, K, N)\n",
        "\n",
        "    Saves:\n",
        "      - per-query grid PNG\n",
        "      - per-image accumulated PNGs\n",
        "      - batch accumulated PNG\n",
        "    All now use upsampled attention maps for better spatial alignment.\n",
        "    \"\"\"\n",
        "    B = len(images)\n",
        "    attn = attn.detach().cpu()     # (B, H, K, N)\n",
        "    heads = attn.size(1)\n",
        "    K = attn.size(2)\n",
        "    N = attn.size(3)\n",
        "\n",
        "    num_q = min(num_queries_vis, K)\n",
        "\n",
        "    # Average attention across heads â†’ (B, K, N)\n",
        "    attn_q = attn.mean(dim=1)\n",
        "\n",
        "    # Per-image grid: [original | per-query heatmaps]\n",
        "    fig, axes = plt.subplots(\n",
        "        B, num_q + 1,\n",
        "        figsize=(3 * (num_q + 1), 3 * B),\n",
        "        squeeze=False\n",
        "    )\n",
        "\n",
        "    for b in range(B):\n",
        "        img_np = np.array(images[b].convert(\"RGB\"))\n",
        "\n",
        "        # Column 0: original image\n",
        "        axes[b, 0].imshow(img_np)\n",
        "        axes[b, 0].set_title(f\"Img {b}\")\n",
        "        axes[b, 0].axis(\"off\")\n",
        "\n",
        "        # Each query slot\n",
        "        for qi in range(num_q):\n",
        "            a_1d = attn_q[b, qi]                  # (N,)\n",
        "            grid = tokens_to_grid(a_1d)           # (h,w)\n",
        "            g = grid.numpy()\n",
        "            g = (g - g.min()) / (g.max() - g.min() + 1e-8)\n",
        "\n",
        "            # ðŸ”¥ Upsample to match full image resolution\n",
        "            g_up = upsample_to_image(g, img_np)\n",
        "\n",
        "            ax = axes[b, qi + 1]\n",
        "            ax.imshow(img_np)\n",
        "            ax.imshow(g_up, cmap=\"jet\", alpha=0.42)\n",
        "            ax.set_title(f\"Q{qi}\")\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    per_query_path = os.path.join(ATTN_OUT_DIR, f\"attn_queries_{batch_tag}.png\")\n",
        "    plt.savefig(per_query_path, dpi=120)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Accumulated attention across queries + heads\n",
        "    # attn: (B, heads, K, N) â†’ avg heads+queries â†’ (B, N)\n",
        "    attn_accum = attn.mean(dim=1).mean(dim=1)\n",
        "\n",
        "    fig2, axes2 = plt.subplots(B, 2, figsize=(6, 3 * B), squeeze=False)\n",
        "    per_image_accum_paths = []\n",
        "\n",
        "    for b in range(B):\n",
        "        img_np = np.array(images[b].convert(\"RGB\"))\n",
        "        a_1d = attn_accum[b]                  # (N,)\n",
        "        grid = tokens_to_grid(a_1d)\n",
        "        g = grid.numpy()\n",
        "        g = (g - g.min()) / (g.max() - g.min() + 1e-8)\n",
        "\n",
        "        # ðŸ”¥ Upsample accumulated attention\n",
        "        g_up = upsample_to_image(g, img_np)\n",
        "\n",
        "        # Batch grid (left = original, right = accumulated)\n",
        "        axes2[b, 0].imshow(img_np)\n",
        "        axes2[b, 0].set_title(f\"Img {b}\")\n",
        "        axes2[b, 0].axis(\"off\")\n",
        "\n",
        "        axes2[b, 1].imshow(img_np)\n",
        "        axes2[b, 1].imshow(g_up, cmap=\"jet\", alpha=0.42)\n",
        "        axes2[b, 1].set_title(\"Accum\")\n",
        "        axes2[b, 1].axis(\"off\")\n",
        "\n",
        "        # Per-image standalone PNG\n",
        "        fig_single, ax_single = plt.subplots(1, 2, figsize=(6, 3))\n",
        "\n",
        "        ax_single[0].imshow(img_np)\n",
        "        ax_single[0].set_title(\"Original\")\n",
        "        ax_single[0].axis(\"off\")\n",
        "\n",
        "        ax_single[1].imshow(img_np)\n",
        "        ax_single[1].imshow(g_up, cmap=\"jet\", alpha=0.42)\n",
        "        ax_single[1].set_title(\"Accumulated\")\n",
        "        ax_single[1].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        single_path = os.path.join(\n",
        "            ATTN_OUT_DIR, f\"attn_accum_img{b}_{batch_tag}.png\"\n",
        "        )\n",
        "        plt.savefig(single_path, dpi=120)\n",
        "        plt.close(fig_single)\n",
        "        per_image_accum_paths.append(single_path)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    accum_grid_path = os.path.join(\n",
        "        ATTN_OUT_DIR, f\"attn_accum_grid_{batch_tag}.png\"\n",
        "    )\n",
        "    plt.savefig(accum_grid_path, dpi=120)\n",
        "    plt.close(fig2)\n",
        "\n",
        "    return {\n",
        "        \"per_query_grid\": per_query_path,\n",
        "        \"per_image_accum_grid\": accum_grid_path,\n",
        "        \"per_image_accum_paths\": per_image_accum_paths,\n",
        "    }\n",
        "\n",
        "\n",
        "# SIMPLE SINGLE-IMAGE HELPER\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_caption(img: Image.Image, max_new_tokens=128):\n",
        "    caps, _ = infer_with_attention([img], batch_tag=\"single\", max_new_tokens=max_new_tokens)\n",
        "    return caps[0]\n",
        "\n",
        "\n",
        "print(\"âœ… Inference + attention system ready.\")\n",
        "print(f\"Attention PNGs will be saved under: {ATTN_OUT_DIR}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV1xx5D6saFs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from glob import glob\n",
        "# test_paths = sorted(glob(\"/content/livis_cache/*.jpg\"))[:15]\n",
        "# imgs = [Image.open(p) for p in test_paths]\n",
        "# captions, paths = infer_with_attention(imgs, batch_tag=\"test_batch\", max_new_tokens=64)\n",
        "# print(\"Captions:\", captions)\n",
        "# print(\"Saved attention:\", paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QATlmsW94SQA",
        "outputId": "80db2af3-ba6b-408e-cbca-eaa1b7c17bfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([\"\\xa0A person in a yellow life jacket sits on a yellow boat near a rocky shoreline, surrounded by trees and a clear sky, indicating a recreational activity or training session. The person wears a helmet and holds a yellow paddle, while the boat is stationary. The sky is clear, and the person's face is visible, indicating\"],\n",
              " {'per_query_grid': '/content/attn_viz/attn_queries_test_batch.png',\n",
              "  'per_image_accum_grid': '/content/attn_viz/attn_accum_grid_test_batch.png',\n",
              "  'per_image_accum_paths': ['/content/attn_viz/attn_accum_img0_test_batch.png']})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imgs = [Image.open(\"muco.png\")]\n",
        "infer_with_attention(imgs, batch_tag=\"test_batch\", max_new_tokens=64)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "062767847423495bb66e01cafb26ac19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a85ee3222a245e5ad6275292567b441": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0f5682b48aad4696a8a58a05de0e5738",
            "style": "IPY_MODEL_55666f14af614893b8ccf1227d11775a",
            "tooltip": ""
          }
        },
        "0f5682b48aad4696a8a58a05de0e5738": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23610da04c044ecd8be542c1c807d122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39f6dd8bb0ba4efb80a59186bc5a465e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_062767847423495bb66e01cafb26ac19",
            "value": "Connecting..."
          }
        },
        "3974289c7526482797c9545f4e9b6831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39f6dd8bb0ba4efb80a59186bc5a465e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b41785cf6c34007815744ae8457eb09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_41b745e8e6f44c5c8318a7e7ad5b8ab7"
          }
        },
        "41b745e8e6f44c5c8318a7e7ad5b8ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4fcb41556b824761a9453ea1758e7652": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55666f14af614893b8ccf1227d11775a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8cdac5c834604f8d8d8149bbb10b4a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fcb41556b824761a9453ea1758e7652",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3974289c7526482797c9545f4e9b6831",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "927e0c7f178f4caeab42da8cc7de9708": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93203f4ef5a6423ca5aa64644eaa0595": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b8bd08c467410bbba5de44a808340e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ad11ac39c241b697d7b2ccad8248a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c905ebd8dfb4bef9a93988b94465a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_93203f4ef5a6423ca5aa64644eaa0595",
            "style": "IPY_MODEL_94ad11ac39c241b697d7b2ccad8248a3",
            "value": true
          }
        },
        "9fdd1b02fd2743c5b5a0387446aedb0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_927e0c7f178f4caeab42da8cc7de9708",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a2020c428cf64c66990c1166d6a026c8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a2020c428cf64c66990c1166d6a026c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eff1deee387e4d46b6a5fd3cf878317c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f569c7b14b70483b83e90a474c6b2cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_93b8bd08c467410bbba5de44a808340e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eff1deee387e4d46b6a5fd3cf878317c",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
